 Welcome to Exclisive days - DevOps - B10
LMS url: https://www.rjpinfotek.com/course/devopsb10
LAB url : https://vlabs.rjpinfotek.com/remote/login?lang=eninternal lab link: http://10.0.15.1/labs/
Please login to LMS start mid-assesment 

java - git - github [VCS]


etherpad : https://etherpad.opendev.org/p/tcs-devops-b10-17nov25
loggen in to lab: shaj,Vaikunda Raja,Karthik Krishnan,Vinoth Baskaran,Himanshu Pandey,Balaji,kishore,Eswari, Mohit Madaan, Kartick H, pathmanathan,karthik, Aditi Varshney,Mahendar

Why Linux
https://w3techs.com/technologies/details/os-linux
https://upload.wikimedia.org/wikipedia/commons/1/1b/Linux_Distribution_Timeline.svg
https://github.com/torvalds/linux
https://kernel.org/
https://www.fedoraproject.org/
https://distrowatch.com/


Linux Basics:
    [root@master ~]# >>>prompt [user@server (homedir)]#(root logged)  -- #(rootuser) /$(normal user)
  [root@master ~]# su - student >login as student
[student@master ~]$ whoami
student
[student@master ~]$ pwd
/home/student >>users home dir
[student@master ~]$ exit
logout
[root@master ~]# pwd
/root >>root home dir
[root@master ~]# 

Lab: login as student : done: shaj,  Vinoth Baskaran,Eswari,Karthik Krishnan , Mohit Madaan, Aditi Varshney ,Balaji, pathmanathan,Himanshu Pandey,Vaikunda Raja,.,kishore, karthik, Katick H,Mahendar
 
  11  whoami
   12  id
   13  command [options] arg
   14  cal
   15  cal mar 2026
   16  cal dec 2026
   17  cal 2026
   18  cal
   19  cal --help
   20  cal -3
   21  cal -3 dec 2026
   22  cal --help
   23  cal -3 dec 2026
   24  cal -3m dec 2026

    
    [student@master ~]$ nano /etc/hosts
[student@master ~]$ su - 
Password:  (root123)
[root@master ~]# nano /etc/hosts
[root@master ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.0.15.1 master master.example.com
10.0.15.2 node1    node1.example.com
10.0.15.3 node2    node2.example.com
ping -c 3 master

LAB: update /etc/hosts file with your systems details (x,y,z is your ips)

    10.0.15.x              master master.example.com

10.0.15.y                  node1     node1.example.com
10.0.15.z            node2                node2.example.com

    Done:  Mohit Madaan ,kishore ,Karthik Krishnan,pathmanathan,Aditi Varshney,Eswari, Vinoth Baskaran,Himanshu Pandey,Chetan Burde,karthik,Vaikunda Raja,Mahesh Meesa,Kartick H, BalajiMahendar


    
Outcome: logging in as user/root/ modify files as root    

Install Application:
    37  yum install htop 
   38  yum list htop
   39  yum list ht*


Lab install ansible
done:Vaikunda Raja, Aditi Varshney, Karthik Krishnan,Eswari, pathmanathan, Vinoth Baskaran,Himanshu Pandey,mohit madaan,kishore, Kartick H , karthik,Balaji,Mahesh Meesa

https://docs.ansible.com/projects/ansible/latest/installation_guide/installation_distros.html

sudoers
ansible inventry
ansible ad-hoc
YAML
ansible playbook
    
    Application: browsing, chatting, emailing,spredsheet, typesetting, graphics
    Program: edge,firefox,chrome,outlook,teams,execel.exe, word.exe bineries
    process: running program - cpu/ram/stor/net - user interactive
    Service: running program - background - trigger (systemD-systemctl)
    deamon: 


 Day 7 - 25Nov25

https://docs.ansible.com/projects/ansible/2.9/modules/modules_by_category.html

Lab: change hostname to nodex, create devops user for automation on node1 and node2
 [student@master ~]$ ssh student@node2
The authenticity of host 'node2 (10.0.15.3)' can't be established.
ED25519 key fingerprint is SHA256:eLU+l4WE/KOm5rit76FusFrAQq94yGSxPElWfdlgGyA.
This host key is known by the following other names/addresses:
    ~/.ssh/known_hosts:1: node1
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added 'node2' (ED25519) to the list of known hosts.
student@node2's password: 
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Sat Nov 15 14:22:16 2025 from ::ffff:10.0.0.2
[student@master ~]$ su -
Password: 
[root@master ~]# hostnamectl hostname node2
[root@master ~]# useradd devops
[root@master ~]# passwd devops
Changing password for user devops.
New password: 
BAD PASSWORD: The password is shorter than 8 characters
Retype new password: 
passwd: all authentication tokens updated successfully.
[root@master ~]# 
logout
[student@master ~]$ 
logout
Connection to node2 closed.
Done: Vaikunda Raja,Karthik Krishnan, AditiVarshney ,BalajiMahendarMohit Madaan,Eswari,Kartick H, Mahesh Meesa, pathmanathan, karthik, Vinoth Baskaran,Himanshu Pandey, kishore
    
    
   LAB: provide devops Sudoers prev. on both node1 and 2
   [student@master ~]$ ssh devops@node2
devops@node2's password: 
Last login: Tue Nov 25 09:49:36 2025 from 10.0.15.1
[devops@node2 ~]$ su -
Password: 
[root@node2 ~]# nano /etc/sudoers.d/admins

    devops ALL=(ALL) NOPASSWD: ALL

[root@node2 ~]# 
logout
[devops@node2 ~]$ useradd x1
useradd: Permission denied.
useradd: cannot lock /etc/passwd; try again later.
[devops@node2 ~]$ sudo useradd x1
 Done:Vaikunda Raja,Eswari,Himanshu Pandey, Aditi Varshney, pathmanathan,Karthik Krishnan,Vinoth Baskaran,KH,Mohit Madaan, Mahesh Meesa, Mahendar, karthik,balaji,kishore
     
    devops key based auth / passwordless auth
  [student@master ~]$ ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key (/home/student/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/student/.ssh/id_rsa
Your public key has been saved in /home/student/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:u+PBRbAbJAVcXwil2v8U+Z4x3RPPoPnhMQYF7NiQbHc student@master
The key's randomart image is:
+---[RSA 3072]----+
|     .o+*+.+o    |
|      .o =*.o.E  |
|        +.o* ..  |
|       o +. oo   |
|      . S . + .. |
|       . +   * ++|
|        + . + O.=|
|        .o o = B.|
|       .o.  . =  |
+----[SHA256]-----+
-----------------------------------------------------------------------------------------------------------------------------
[student@master ~]$ ssh-copy-id devops@node1
/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
devops@node1's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'devops@node1'"
and check to make sure that only the key(s) you wanted were added.

[student@master ~]$ ssh devops@node1
Last login: Tue Nov 25 10:29:26 2025 from 10.0.15.1
  
For node 2   
[student@master ~]$ ssh-copy-id devops@node2
/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
devops@node1's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'devops@node2'"
and check to make sure that only the key(s) you wanted were added.


LAB: devops user must log in to node1 and node2 without password

Done : Mohit Madaan,Vaikunda Raja, Vinoth Baskaran,Karthik Krishnan,Mahendar,KH,Eswari,Himanshu Pandey,pathmanathan, karthik,Balaji,kishore

[student@master ~]$ ssh devops@node1 hostname -i
fe80::be24:11ff:feb1:546f%ens18 10.0.15.2
[student@master ~]$ ssh devops@node2 hostname -i
fe80::be24:11ff:fe8a:1c8%ens18 10.0.15.3

LAB : create ansible inventory file by adding node1 in web group, node2 in db group
mkdir prod
cd prod
nano inventory.ini
[web]
node1
[db]v
node2

ansible all -i inventory.ini --list-hosts
Done:Himanshu Pandey, Vinoth Baskaran,Balaji,Vaikunda Raja,Mohit Madaan,EswariMahendar,pathmanathan,karthik,kishore


LAB: setup ansible config

[student@master prod]$ cat ansible.cfg 
[defaults]
#inventory file path-contains mananaged hosts list
inventory=inventory.ini
#connect to remote system
remote_user=devops
#ask every time password
ask_pass=false
#dont chek remote host key
host_key_checking=false

[privilege_escalation]
become=true
become_method=sudo
become_user=root
become_ask_pass=false

ansible web -m command -a 'id' -k -b -u

use copy module
LAB: copy /etc/hosts to allservers /tmp/hosts.file
ansible allserver -m copy -a 'src=/etc/hosts dest=/tmp/hosts.file'
ansible allserver -m command -a 'ls -l /tmp/hosts.file'
done: KH, Vinoth Baskaran mohit madaan,Himanshu Pandey,pathmanathan,Karthik Krishnan,Balaji,Vaikunda Raja,Eswari,karthik,kishore
    

LAB: install vscode
intro to YAML
#YAML Intro
# <key>: <value>
team: csk #string
city: "chennai"
year: 2008 #numeric
is_current_champ: true #false, 0, 1, yes , no #boolean
src: /etc/hosts
dest: /tmp/hosts
port: 8090

#list
servers_list: ["server1","server2","s3"]
servers_list:
  - server1
  - server2
  - s3

#dictionary
servers_data:
  - name: webserver
    ip: 10.0.15.1
    role: apache
  - name: dbserver
    ip: 10.0.15.3
    role: mysql
  - name: mailserver
    ip: 1.2.3.4
    role: smtp

Done: pathmanathan,Eswari,Balaji,Karthik Krishnan,kishore

   
 Day 8- 26Nov25
https://web.devopstopologies.com/ 

install vscode extension YAML and ansible from RedHat   

[student@master prod]$ cat ./2.usercreate.yml
#create user called sales1
- name: p1 - create user account in all managed node
  hosts: allservers
  tasks:
    - name: create user sales1
      ansible.builtin.user:
        name: sales1
        state: present
        comment: "sales manager"
#  ansible-playbook ./2.usercreate.yml --syntax-check
#  ansible-playbook ./2.usercreate.yml
   
LAB: create diskfree report save in /tmp/diskreport.txt on allservers group
use shell module "df -h > /tmp/diskreport.txt"
get the content using "cat /tmp/diskreport.txt"
Done: Balaji,Mohit Madaan, KH Unable to do lab today having sys issues, Eswari, Himanshu Pandey ,Vaikunda raja,Vinoth Baskaran,Karthik Krishnan, Mahesh Meesa,pathmanathan, karthik


  [student@master prod]$ cat ./4.webserver.yml
#install web server, create / copy homepage, enable service autostart, start service, open firewall
- name: install and configure apache web server
  hosts: web
  tasks:
    - name: install httpd app
      tags:
        - install
        - web
      dnf:
        name: httpd
        state: latest
    - name: create/copy home page
      #ignore_errors: false
      copy:
        #src: homepage.html #create hp in master node
        content: "This web page created using ansible PB task"
        dest: /var/www/html/index.html
    - name: start service and enable
      tags:
        - configure
        - web
      ansible.builtin.service:
        name: httpd
        state: started
        enabled: true
    - name: open firewall port 80
      ansible.posix.firewalld:
        service: http
        #port: 80/tcp
        state: enabled
        immediate: yes
        permanent: 1 

   
   LAB: install mariadb app in db group
   appname is mariadb-server
   open port no: 3306/tcp
   service name: mariadb.service
   
   
# Maria db Install 
- name: Maria db installation in the db servers
  hosts: db
  tasks: 
    - name: install maria db 
      tags:
        - install 
        - db
      dnf: 
        name: mariadb-server
        state: latest
    # - name: create/copy db page
    #   #ignore_errors: false
    #   copy:
    #     #src: homepage.html #create hp in master node
    #     content: "This maria db created using ansible PB task"
    #     dest: /var/www/html/index.html
    - name: start service and enable
      tags:
        - configure
        - db
      ansible.builtin.service:
        name: mariadb.service
        state: started
        enabled: true
    - name: open firewall port 3306
      ansible.posix.firewalld:
        service: mysql
        #port: 3306/tcp
        state: enabled
        immediate: yes
        permanent: 1 
  
  
   
   done: Balaji, MOhit Madaan,Eswari, Vinoth Baskaran, Karthik Krishnan, Himanshu ,Vaikunda Raja,pathmanathan,karthik
       
  [student@master prod]$ cat ./6.varbaseddeploy.yaml
#try consume variables overtime dont touch this file
- name: "deploy {{ app }}"
  hosts: "{{group}}"
  # vars:
  #   app: httpd
  #   appservice: httpd
  #   appport: 80/tcp
  vars_files: app-deploy
  tasks:
    - name: "install {{ app }} in {{group}} nodes" 
      dnf:
        name: "{{app}}"
        state: latest
    - name: "start {{appservice}} service"
      service:
        name: "{{appservice}}"
        state: started
        enabled: true
    - name: "open port for {{app}} of port no {{appport}}"
      ansible.posix.firewalld:
        #service: mysql
        port: "{{appport}}"
        state: enabled
        immediate: yes
        permanent: 1
    
    
[student@master prod]$ cat ./app-deploy
app: dialog
appservice: httpd
appport: 80/tcp
group: db

    LAB: change varibale in command line
    ansible-playbook ./6.varbaseddeploy.yaml -e app=tftp -e appservice=httpd -e appport=21/tcp
    done: Balaji , Himanshu Mohit Madaan,Vaikunda Raja,Vinoth Baskaran, Mahesh Meesa, Karthik Krishnan,Eswari
         
 use loop in playbook
    
create user accounts with password
[student@master prod]$ cat ./7.multi-user-password.yml
- name: create multiple users
  hosts: allservers
  #vars_files: userlist.txt
  vars_files: vaulteduserlist.txt

  # vars:
  #   vusers:
  #     - sales12
  #     - salesmanager10
  #     - sales11
  tasks:
    - name: create users
      no_log: true
      user:
        name: "{{item.uname}}"
        state: present
        comment: "{{item.uname}}"
        password: "{{ item.upass | password_hash('sha512') }}"
      #with_items:
      loop: "{{vusers}}"
        # - sales1
        # - sales2
        # - salesmanager 
 
 protect the userlist file with vault password:
   215  cp userlist.txt vaulteduserlist.txt 
  216  ansible-vault encrypt vaulteduserlist.txt 
  217  cat vaulteduserlist.txt 
  218  ansible-vault view vaulteduserlist.txt 

use vaulted password in plybook execution
  220  ansible-playbook ./7.multi-user-password.yml
  221  ansible-playbook ./7.multi-user-password.yml --ask-vault-password
 
 Day 9 - 27Nov25
 IAC -Terrform / Ansible
 
 GatherFacts using adhoc
 ansible web -m setup
 
 When Condition
 [student@master prod]$ cat ./8.when-deploy.yml
#install web server, create / copy homepage, enable service autostart, start service, open firewall
- name: install and configure apache web server
  hosts: web
  tasks:
    - name: install httpd app
      dnf:
        name: httpd
        state: latest
      when: ansible_distribution == "Rocky2"
    - name: create/copy home page
      #ignore_errors: false
      copy:
        #src: homepage.html #create hp in master node
        content: "This web page created using ansible PB task"
        dest: /var/www/html/index.html
    - name: start service and enable
      tags:
        - configure
        - web
      ansible.builtin.service:
        name: httpd
        state: started
        enabled: true
    - name: open firewall port 80
      ansible.posix.firewalld:
        service: http
        #port: 80/tcp
        state: enabled
        immediate: yes
        permanent: 1
 
 
 Lab: install dovecot if  "ansible_memtotal_mb" more than 4GB RAM
current gathered fact:     ansible_memtotal_mb  3655
 
 done: Balaji,pathmanathan.Kartick H, Vinoth Baskaran, Karthik Krishnan,Vaikunda Raja
[student@master prod]$ cat ./9.memfacts.yml
---
- name: deploy dovecot app in systems with more than 4GB
  hosts: web
  tasks:
    - name: install dovecot
      package: 
        name: dovecot
        state: absent
      when: ansible_memtotal_mb <= 4096
    - name: install apache in redhat family OSes
      dnf:
        name: httpd
        state: latest
      when: ansible_os_family == "Redhat"
    - name: install nginx in debian family OSes
      apt:
        name: nginx
        state: latest
      when: ansible_os_family == "debian"
...


[student@master prod]$ cat ./10.import-playbook.yml
#import playbooks
- name: deploy frontend webserver
  ansible.builtin.import_playbook: "4.webserver.yml"
- name: deploy backend dbservers
  ansible.builtin.import_playbook: "5.mariadb-install.yml"
     
 [student@master prod]$ tree ./tasks/
./tasks/
├── copy.yml
├── firewallport.yml
├── install.yml
└── service.yml

[student@master prod]$ cat ./tasks/install.yml
- name: install httpd app
  tags:
    - install
  dnf:
    name: "{{ pkg }}"
    state: latest
 [student@master prod]$ cat ./tasks/copy.yml
- name: copy as per need  
  copy:
    #src: homepage.html #create hp in master node
    content: "This web page created using ansible PB task"
    dest: /tmp/test.html
- name: generate report
  shell: uptime > /tmp/uptime.txt
 
 [student@master prod]$ cat ./tasks/service.yml
- name: start service and enable
  tags:
    - configure
    - service
  ansible.builtin.service:
    name: "{{appservice}}"
    state: started
 
[student@master prod]$ cat ./tasks/firewallport.yml
- name: "open firewall port {{fport}}"
  ansible.posix.firewalld:
    port: "{{fport}}/tcp"
    state: enabled
    immediate: yes
    permanent: 1
 
 [student@master prod]$ cat ./11.includeimport-tasks.yml
- name: use include or import tasks
  hosts: web
  tasks:
    - name: get system ip 
      shell: hostname -i
    - name: include task on install
      include_tasks: tasks/install.yml
      vars: 
        pkg: httpd
    - name: import tasks of copy.yml 1000tasks
      import_tasks: tasks/copy.yml
    - name: inclued task service
      include_tasks: tasks/service.yml
      vars:
        appservice: httpd 
 
 Done: Balaji, Mohit Madaan, Karthik Krishnan, Vinoth Baskaran, Mahesh Meesa,Vaikunda Raja, Himanshu , Kartick H,pathmanathan
     
     
    LAB: open port 8080 on all systems using import task

Done: Mohit Madaan, Himanshu , Vinoth Baskaran,Kartick H,Karthik Krishnan, Vaikunda Raja,
 
 ROLES : 
 playbook 
 tasks\....
 templates\..
 files\...
 handlers\...
 
   280  ansible-galaxy 
  281  ansible-galaxy  role
  282  ansible-galaxy  role -h
  283  ansible-galaxy  role init web
  284  tree ./web/
  285  ansible-galaxy  role init db
  286  ansible-galaxy  role init lobapp1
  287  cd ..
  288  ansible-galaxy roles list
  289  ansible-galaxy role list
  290  ansible-play
  291  ansible-playbook ./12.roles-web.yml
  292  ansible-galaxy role install buluma.httpd
  293  ansible-galaxy role list
  
  [student@master prod]$ cat ./12.roles-web.yml
- name: use roles to deploy app
  hosts: web
  vars:
    pkg: nginx
    appservice: nginx
    fport: 80
  roles:
    - web
 
 LAB: create db role useit in playbook
 
 
 Day 10 - 28Nov25
 [student@master prod]$ cat ./14.updatehosts.yml
- name: copy hosts file to all managed nodes
  hosts: allservers
  tasks:
    - name: copy hosts file
      copy:
        src: hostsfile
        dest: /etc/hosts
        backup: yes
    - name: automation account auto1
      ansible.builtin.user:
        name: auto1
        state: present
    - name: grant sudo by adding in /etc/sudoers.d/admins
      ansible.builtin.lineinfile:
        path: /etc/sudoers.d/admins
        line: "auto1 ALL=(ALL) NOPASSWD: ALL"
        state: present
        create: yes
    - name: enable keybased auth/passwordless auth (ssh-copy-id)
      ansible.posix.authorized_key:
        user: auto1
        state: present
        key: "{{ lookup('file', '/home/student/.ssh/id_rsa.pub') }}"

# ansible allservers -m copy -a 'src=hostsfile dest=/etc/hosts backup=yes' -u targetusername -k -b
# ansible allservers -m user -a 'name=auto2 state=present' -u targetusername -k -b
# ansible allservers -m lineinfile -a "path=/etc/sudoers.d/admins line='auto2 ALL=(ALL) NOPASSWD: ALL' state=present" -u targetusername -k -b
# ansible allservers -m ansible.posix.authorized_key -a 'user=auto2 key='"{{ lookup('file', '/home/student/.ssh/id_rsa.pub') }}" state=presnt' -u targetusername -k -b
 
 Provisioning - ConfigManagement
 Day1+
 
 Docker 
 https://docs.docker.com/engine/install/
   sudo dnf remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine`
                  
sudo dnf -y install dnf-plugins-core
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

sudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo systemctl enable --now docker


LAB: install docker on node2
done:    Kishore, Karthik Krishnan,EswariMohit Madaan,Himanshu, karthik, Balaji,Vaikunda Raja,Kartick H, Vinoth Baskaran,Pathmanathan
    
    
    Lab: use docker pull to download busybox, alpine, ubuntu, almalinux
    docker pull imagename
    
    docker run -it busybox /bin/sh

        hostname -i

         mkdir /somename
         cal 2024 > /somename/cal2024.txt
         exit
   docker ps -a
   docker diff containerID
   docker commit containerID newimagename:version
   docker images

   
   
   Done: Balaji , Himanshu , Vaikunda raja,Eswari, kishore, vinoth baskaranMohit Madaan,Karthik Krishnan,Kartick,pathmanathan
   
   
  Day11 - 1Dec25 
     99  docker run -it --name bbapp1 busybox /bin/sh 
  100  docker ps
  101  docker ps -a
  102  docker diff bbapp1
  103  docker commit bbapp1 shajahanak/bbdecimage
  104  docker images
  105  docker push shajahanak/bbdecimage:latest 
  106  docker login
  107  docker push shajahanak/bbdecimage:latest 

    
LAB: 
    create hub.docker.com - signup / account
    launch busybox image as container [    docker run -it --name somename busybox /bin/sh ]
    create some folders and files
    exit from shell(which exits the container]
    use doker diff to find difference
    use docker commit to save the changes as image
    save image name youraccountname/imagename format
    docker login youraccountname
    docker push youraccountname/imagename
    login to dockerhub, make public view of your image
    
    done: Karthik Krishnan, Vinoth Baskaran,Himanshu Pandey, MohitMadaan, Kishore, Balaji,Eswari,Vaikunda raja,mahesh meesa,Kartick H,pathmanathan
    
        share your imagenames here:
            pathmanathanb/busybox-custom
             docker pull kishore520741/bbdecimage
             docker pull himanshu2198/myfirstimage
             docker pull rkarthik4/testimage1
             vinothbaskaran1985/bbdecimage
            docker pull mohit123456789/dectestcal
            docker pull vedikabalaji/bbdecimage:latest
            docker pull eswari1997/busyboxdec
            vaikundarajas/rajaapp1
            maheshmeesa/dec-image-1
            
docker pull kartickh/bbdeplytodockerhub
            https://hub.docker.com/repositories/mahendardevops?_gl=1*o7adm2*_ga*MTc4NDM3MjQzNS4xNzY0NTY0MDY5*_ga_XJWPQMJYHQ*czE3NjQ1NjQwNjkkbzEkZzEkdDE3NjQ1NjYxNDYkajYwJGwwJGgw
    
    docker pull kishore520741/texteditorimages:1.0.1

http://10.0.151/labs/vscode.rpm
To launch VScode with root account

code /directory-to-open --user-data-dir='.' --no-sandbox    
  mkdir docker
  code /root/docker --user-data-dir='.' --no-sandbox  
    
https://www.docker.com/101-tutorial

Docker Images Build
cat Dockerfile
FROM alpine
RUN apk add vim
RUN apk add nano
RUN apk add elinks
RUN mkdir /myappdir
RUN cal 2026 > /myappdir/cal2026.txt

docker build -t youraccountname/imagename:version . >>>>. points to Dockerfile
docker images
docker push yourbultimage >>>to dockerhub
done: Karthik Krishnan,Balaji, Vinoth,Eswari, Mohit Madaan,Vaikunda RajaMahendar
    
use almalinux + httpd + create homepage
Done: Balaji, Karthik Krishnan,

student issue: docker --- unix:socker error in docker
userlogins: devops 68, Devops-63 - socket error, I am able to login via student


Day 12 - 2Dec25

To launch VSCODE USING Root 

code --user-data-dir="~/.vscode-root" --no-sandbox

LAB: use debian / almalinux (httpd)
create image using dockerfile
FROM debian:stable
LABEL authors="devopst1"
RUN apt-get update && apt-get install -y --force-yes apache2

docker build -f filename -t uraccount/imagename:1.0.1 .
done:Mohit Madaan,Karthik Krishnan,BalajiMahendar,pathmanathan,Vinoth Baskaran,Himanshu Pandey

Balaji: - docker pull vedikabalaji/debimage:1.0.1
Mahendar : docker pull mahendardevops/debianfileimage:1.0.1


https://raw.githubusercontent.com/VedikaBalaji/DevOps_Labs/refs/heads/main/testfile.txt
    
https://raw.githubusercontent.com/abdulshajahan/ansiblenov25/refs/heads/main/testfile.txt
-----------------------------------------------------------------------------------------------
FROM debian:stable
LABEL authors="devopst1"
#use run directive to execute commands with &&
RUN apt-get update && apt-get install -y --force-yes apache2
RUN mkdir /myapp
WORKDIR /myapp
COPY date.sh .
ADD https://raw.githubusercontent.com/VedikaBalaji/DevOps_Labs/refs/heads/main/testfile.txt ./balaji.txt
RUN chmod 777 date.sh
ENV MYAPP=/myapp
WORKDIR /myapp
CMD [ "./date.sh" ]
ENTRYPOINT [ "date" ]

Use the following dockerfile to build image and push dockerhub

# Use AlmaLinux 9 as base image
FROM almalinux:9

# Set environment variables
ENV DOCUMENT_ROOT=/var/www/html \
    HTTPD_CONF_DIR=/etc/httpd/conf \
    HTTPD_CONF_D_DIR=/etc/httpd/conf.d

# Install Apache HTTPd and clean cache
RUN dnf update -y && \
    dnf install -y httpd && \
    dnf clean all && \
    rm -rf /var/cache/dnf

# Create a sample homepage
RUN echo 'Webserver app deployed using Docker Build' > ${DOCUMENT_ROOT}/index.html

# Set proper permissions
RUN chown -R apache:apache ${DOCUMENT_ROOT} && \
    chmod -R 755 ${DOCUMENT_ROOT}

# Expose port 80
EXPOSE 80

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost/ || exit 1

# Start Apache in foreground mode
CMD ["httpd", "-D", "FOREGROUND"]

Done : Mohit madaanMahendar,Karthik Krishnan,
-------------------------------------------------------------------------------------------------------------

cat date.sh
#!/bin/bash
echo `date` $@ >> log.txt;
cat log.txt;
#save as date.sh set chmod 777 script

docker entrypoint works as mentioned below :-
ENTRYPOINT ["ls"]   
#you can give uptime here and -p in docker run command

[root@node2 docker]# docker run vedikabalaji/debimage:1.0.5 -ltr 
total 8
-rw-------. 1 root root 22 Jan  1  1970 balaji.txt
-rwxrwxrwx. 1 root root 89 Dec  2 05:06 date.sh
[root@node2 docker]# 


LAB: create image and run it on port 9191
    
# Use AlmaLinux 9 as base image
FROM almalinux:9

# Set environment variables
ENV DOCUMENT_ROOT=/var/www/html \
    HTTPD_CONF_DIR=/etc/httpd/conf \
    HTTPD_CONF_D_DIR=/etc/httpd/conf.d

# Install Apache HTTPd and clean cache
RUN dnf update -y && \
    dnf install -y httpd && \
    dnf clean all && \
    rm -rf /var/cache/dnf

# Create a sample homepage
RUN echo 'Webserver app deployed using Docker Build' > ${DOCUMENT_ROOT}/index.html

# Set proper permissions
RUN chown -R apache:apache ${DOCUMENT_ROOT} && \
    chmod -R 755 ${DOCUMENT_ROOT}

# Expose port 80
EXPOSE 80

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost/ || exit 1

# Start Apache in foreground mode
CMD ["httpd", "-D", "FOREGROUND"]

cmd ref
  284  docker build -f 2.1almaapache-simple.Dockerfile -t almaaweb:1.1.1 .
  285  docker run -d -p 8012:80 almaaweb:1.1.1
  286  docker ps

use curl / firefox to reach the app

Done: Mohit Madaan,Karthik Krishnan,Vinoth Baskaran,Vaikunda Raja,Himanshu Pandey, Mahesh Meesa

FROM httpd:2.4
#create public-html in docker node and create index.html
COPY ./public-html/ /usr/local/apache2/htdocs/
# $ docker build -t my-apache2 .
# $ docker run -dit --name my-running-app -p 8080:80 my-apache2

FROM nginx
#COPY hp.html /usr/share/nginx/html/index.html
ADD https://raw.githubusercontent.com/abdulshajahan/ansiblenov25/main/hp.html /usr/share/nginx/html/index.html
RUN chmod 644 /usr/share/nginx/html/index.html
EXPOSE 80
# Nginx runs in foreground by default in official image
# No need for CMD as it inherits from base image
# But we can explicitly state it for clarity:
CMD ["nginx", "-g", "daemon off;"]

Lab: create httpd or nginx image and run it on port 9099
done:Himanshu Pandey, Balaji, mohit madaan, Mahesh Meesa,Karthik Krishnan,Vaikunda raja,Vinoth Baskaran
http://10.0.15.93:9091
http://10.0.15.63:9091 --this is IP from my machine.

httpd mapping path   local:containerspath              -v     ./public-html/:/usr/local/apache2/htdocs/
NGINX mapping path   local:containerspath              -v  ./publix-html:/usr/share/nginx/html/
LAB: map local path to container path with volume mapping
done:Himanshu Pandey,Vaikunda raja,Balaji, Mohit Madaan, Mahesh Meesa, Vinoth Baskaran
    

CMD (Command)

    Provides default executable and arguments

    Can be completely overridden at runtime

    Three forms:

    CMD ["executable","param1","param2"]  # exec form (recommended)

    CMD ["param1","param2"]               # as default parameters to ENTRYPOINT

    CMD command param1 param2             # shell form


ENTRYPOINT

    Defines the main executable

    Two forms:

    ENTRYPOINT ["executable", "param1", "param2"]  # exec form (recommended)

    ENTRYPOINT command param1 param2               # shell form


    Can be overridden with --entrypoint flag only

    Container becomes an "executable" with ENTRYPOINT



Interaction Between CMD and ENTRYPOINT
ENTRYPOINT                       CMD                          Result when container runs                 Override example
Not set                               CMD ["/bin/ls", "-l"]     /bin/ls -l                                                docker run myimage /bin/sh → runs /bin/sh
ENTRYPOINT ["ls"]           Not set                         ls                                                            docker run myimage -la          → ls -la
ENTRYPOINT ["ls"]           CMD ["-l"]                     ls -l                                                         docker run myimage -la          → ls -la
ENTRYPOINT ["ls"]           CMD ["-l", "-a"]              ls -l -a                                                    docker run myimage -lh          → ls -lh
Important: When both are set, CMD becomes arguments to ENTRYPOINT!

Is your container a single-purpose tool/application?
    ├── Yes → Use ENTRYPOINT (with optional CMD for defaults)
    └── No → Do you need a default command?
            ├── Yes → Use CMD
            └── No → Don't use either (let user specify at runtime)


https://docs.docker.com/get-started/docker-concepts/running-containers/publishing-ports/
https://docs.docker.com/get-started/docker-concepts/building-images/writing-a-dockerfile/

Day 13 - 3Dec25
https://hub.docker.com/_/mariadb#where-to-store-data

  356  docker volume create vol1
  357  docker volume ls
  358  docker volume create vol2
  359  docker volume ls
  360  docker run -it --mount type=volume,source=vol1,target=/vol1fromhost alpine sh
  361  cd /var/lib/docker/volumes
  362  sudo cd /var/lib/docker/volumes
  363  ls
  364  pwd
  365  sudo cd /var/lib/docker/volumes
  366  su -
  367  mkdir test123
  368  touch test123/bindmounttest.txt
  369  docker run -it -v ./test123:/bindvol alpine sh
  370  docker ps
  371  docker ps -a
  372  docker inspect 0ea9c98c52ef
  373  docker ps -a
  374  docker inspect 0531bed7dafb
  375  docker network ls
  376  docker network inspect bridge
  377  docker ps
  378  docker exec -it 99debf797582 sh
  379  curl http://172.17.0.3
  380  docker run -d nginxdemos/hello:0.4-plain-text 
  381  docker inspect 0830
  382  curl http://172.17.0.2
  383  docker run -d nginxdemos/hello:0.4-plain-text 
  384  docker inspect 8fff
  385* docker inspect 08
  386  curl http://172.17.0.10
  387  curl http://172.17.0.2
  388  docker images

LAB: create / use nginxdemos/hello:0.4-plain-text , nginxdemos/hello:latest
get its ip add - network type
docker pull nginxdemos/hello:0.4-plain-text
docker pull nginxdemos/hello

cont1(172.17.0.x) >>bridge(172.17.0.0/16)
from docker host http://172.17.0.x
reach from outside (HostIP portmapp)
docker run -d -p hostport:containerport imagename

Done: Mohit Madaan, Balaji,Vaikunda Raja,Karthik Krishnan, Mahesh Meesa, karthik

Install Jenkins in docker system

sudo wget -O /etc/yum.repos.d/jenkins.repo \
    https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
sudo yum upgrade
# Add required dependencies for the jenkins package
sudo yum install fontconfig java-21-openjdk
sudo yum install jenkins
sudo systemctl daemon-reload

sudo usermod -a -G docker jenkins
sudo chown jenkins:docker /var/run/docker.sock
sudo chmod 660 /var/run/docker.sock

sudo systemctl enable jenkins
sudo systemctl start jenkins
sudo systemctl status jenkins

Browse to http://localhost:8080 (or whichever port you configured for Jenkins when installing it) and wait until the Unlock Jenkins page appears.

after inital setup - install docker plugins

https://www.jenkins.io/doc/book/installing/linux/#red-hat-centos

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

dckr_pat_LYGud5W97ldJMoLclY8FXYH8SOQ



pipeline {
    agent any

    environment {
        GIT_REPO = "https://github.com/abdulshajahan/tfrepo1.git"
        GIT_CREDENTIALS_ID = "PAT1"  // Add this in Jenkins if repo is private
        DOCKER_CREDENTIALS_ID = "DOC-PAT" //add this in Jenkins
        APP_NAME = 'docker.io/shajahanak/webmar25'

    }

    stages {
        stage('Clone Repository') {
            steps {
                sh 'rm -rf repo && git clone ${GIT_REPO} repo'
            }
        }

        stage('Build Docker Image') {
            steps {
                script {
                    sh "cd repo && docker build -t ${APP_NAME} ."
                }
            }
        }

        stage('Login to Docker Hub') {
            steps {
                script {
                    withDockerRegistry([credentialsId: DOCKER_CREDENTIALS_ID, url: "https://index.docker.io/v1/"]) {
                        sh "echo 'Docker Login Successful'"
                    }
                }
            }
        }

        stage('Push Image to Docker Hub') {
            steps {
                script {
                    sh "docker push ${APP_NAME}"
                }
            }
        }

        stage('Cleanup') {
            steps {
                sh "docker rmi ${APP_NAME}:latest"
            }
        }
    }
}


Day 14 - 4Dec25
Good Morning
Pls take copy of your docker lab files(node2) to master(ansible)/googledrive/onedrive

zip all the required files, use scp to copy to master
scp docker-labs.zip student@master:/home/student/


Docker
Dcoker-cli >>>>DockerEngine(Host)<<<<<<<<<<<<DockerHub[public]
Container
run, start,stop,rm,create.pause, exec -p -v
Image
pull,push
Build - docker built -t repo:tag -f dockerfilename .




Deploy K8s master
#!/bin/bash 
 
# ============================================================ 
# Kubernetes + Containerd Installation Script for Rocky Linux 9 
# Author: Shaj 
# Version: 2.1 
# ============================================================ 
 
set -e # Exit immediately if a command exits with a non-zero status 
 
KUBERNETES_VERSION="1.32" 
 
echo "==============================================" 
echo " Starting Kubernetes $KUBERNETES_VERSION with Containerd setup" 
echo "==============================================" 
 
# ============================================================ 
# Step 1: Disable Firewall and Swap 
# ============================================================ 
echo "[1/9] Disabling firewalld and swap..." 
systemctl disable --now firewalld || true 
swapoff -a 
sed -i '/swap/s/^/#/' /etc/fstab 
echo "✅ Firewall disabled and swap turned off" 
 
# ============================================================ 
# Step 2: Enable required kernel modules and sysctl parameters 
# ============================================================ 
echo "[2/9] Enabling kernel modules and sysctl parameters for networking..." 
 
# Load kernel modules 
cat <<EOF | tee /etc/modules-load.d/k8s.conf 
overlay 
br_netfilter 
EOF 
 
modprobe overlay 
modprobe br_netfilter 
 
# Configure sysctl parameters 
cat <<EOF | tee /etc/sysctl.d/k8s.conf 
net.bridge.bridge-nf-call-iptables = 1 
net.bridge.bridge-nf-call-ip6tables = 1 
net.ipv4.ip_forward = 1 
EOF 
 
sysctl --system 
echo "✅ Kernel modules and sysctl parameters configured successfully" 
 
# ============================================================ 
# Step 3: Install required dependencies 
# ============================================================ 
echo "[3/9] Installing dependencies..." 
dnf install -y dnf-plugins-core container-selinux curl 
echo "✅ Dependencies installed successfully" 
 
# ============================================================ 
# Step 4: Configure Kubernetes repository 
# ============================================================ 
echo "[4/9] Configuring Kubernetes repo for $KUBERNETES_VERSION..." 
cat <<EOF | tee /etc/yum.repos.d/kubernetes.repo 
[kubernetes] 
name=Kubernetes 
baseurl=https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/rpm/ 
enabled=1 
gpgcheck=1 
gpgkey=https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/rpm/repodata/repomd.xml.key 
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni 
EOF 
echo "✅ Kubernetes repo configured successfully" 
 
# ============================================================ 
# Step 5: Install and configure Containerd from Docker repo 
# ============================================================ 
echo "[5/9] Installing and configuring Containerd..." 
 
# Add Docker repository 
dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 
 
# Install containerd 
dnf install -y containerd.io 
 
# Generate default containerd config and enable systemd cgroups 
mkdir -p /etc/containerd 
containerd config default | tee /etc/containerd/config.toml 
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml 
 
# Start and enable containerd 
systemctl daemon-reload 
systemctl enable containerd --now 
 
# Verify containerd is working 
if systemctl is-active --quiet containerd; then 
    echo "✅ Containerd installed and running successfully" 
else 
    echo "❌ Containerd failed to start" 
    systemctl status containerd 
    exit 1 
fi 
 
# ============================================================ 
# Step 6: Install Kubernetes components 
# ============================================================ 
echo "[6/9] Installing kubelet, kubeadm, and kubectl..." 
dnf install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 
echo "✅ Kubernetes components installed successfully" 
 
# ============================================================ 
# Step 7: Enable kubelet service 
# ============================================================ 
echo "[7/9] Enabling kubelet service..." 
systemctl enable kubelet.service 
echo "✅ kubelet service enabled" 
 
# ============================================================ 
# Step 8: Initialize Kubernetes control plane 
# ============================================================ 
echo "[8/9] Initializing Kubernetes cluster..." 
 
# Configure kubeadm to use containerd 
cat <<EOF | tee /tmp/kubeadm-config.yaml 
apiVersion: kubeadm.k8s.io/v1beta3 
kind: InitConfiguration 
nodeRegistration: 
  criSocket: "unix:///var/run/containerd/containerd.sock" 
--- 
apiVersion: kubeadm.k8s.io/v1beta3 
kind: ClusterConfiguration 
networking: 
  podSubnet: "192.168.0.0/16" 
EOF 
 
kubeadm init --config=/tmp/kubeadm-config.yaml | tee /root/bootstrap.txt 
 
# Check if initialization was successful 
if [ $? -eq 0 ]; then 
    echo "✅ Kubernetes cluster initialized successfully" 
else 
    echo "❌ Kubernetes cluster initialization failed" 
    exit 1 
fi 
 
# ============================================================ 
# Configure kubectl for both root and student user 
# ============================================================ 
 
echo "Configuring kubectl for root user..." 
mkdir -p /root/.kube 
cp -i /etc/kubernetes/admin.conf /root/.kube/config 
chown root:root /root/.kube/config 
 
echo "Configuring kubectl for student user..." 
mkdir -p /home/student/.kube 
cp -i /etc/kubernetes/admin.conf /home/student/.kube/config 
chown student:student /home/student/.kube/config 
chmod 600 /home/student/.kube/config 
 
# Add KUBECONFIG to student's bashrc 
echo "export KUBECONFIG=/home/student/.kube/config" >> /home/student/.bashrc 
 
# ============================================================ 
# Step 9: Configure kubectl and networking 
# ============================================================ 
echo "[9/9] Configuring kubectl and networking..." 
 
# Configure kubectl for root user 
mkdir -p $HOME/.kube 
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 
chown $(id -u):$(id -g) $HOME/.kube/config 
 
# Install Calico CNI 
echo "Installing Calico CNI..." 
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.30.3/manifests/calico.yaml 
 
echo "✅ Calico network plugin installed" 
 
# Wait for pods to be ready 
echo "Waiting for system pods to be ready..." 
sleep 60 
 
echo "✅ Kubernetes Pod Status:" 
kubectl get pods -A 
 
echo "✅ Kubernetes Node Status:" 
kubectl get nodes 
 
echo "==============================================" 
echo " Kubernetes $KUBERNETES_VERSION + Containerd setup complete!" 
echo "==============================================" 
echo "" 
echo "To start using your cluster:" 
echo "1. Run: kubectl get pods -A" 
echo "2. To join worker nodes, use the command from: /root/bootstrap.txt" 
echo "3. Remove taint for single-node cluster: kubectl taint nodes --all node-role.kubernetes.io/control-plane-" 
echo "=============================================="




Control plane
Protocol    Direction    Port Range    Purpose    Used By
TCP    Inbound    6443    Kubernetes API server    All
TCP    Inbound    2379-2380    etcd server client API    kube-apiserver, etcd
TCP    Inbound    10250    Kubelet API    Self, Control plane
TCP    Inbound    10259    kube-scheduler    Self
TCP    Inbound    10257    kube-controller-manager    Self

Although etcd ports are included in control plane section, you can also host your own etcd cluster externally or on custom ports.
Worker node(s)
Protocol    Direction    Port Range    Purpose    Used By
TCP    Inbound    10250    Kubelet API    Self, Control plane
TCP    Inbound    10256    kube-proxy    Self, Load balancers
TCP    Inbound    30000-32767    NodePort Services†    All
UDP    Inbound    30000-32767    NodePort Services†    All

https://v1-32.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

LAB: deploy K8s master
kubectl get nodes >>>master ready
kubectl get pods -A
Done:Mahendar,Mohit Madaan Vinoth Baskaran,Karthik Krishnan,Himanshu Pandey, Mahesh Meesa, karthik,Balaji,Vaikunda raja,Eswari.Kartick H

    
    export KUBECONFIG=/etc/kubernetes/admin.conf

Shortcut for kubectl :- no need to type full command kubectl
    alias k='kubectl'
[root@master k8s]# k get nodes
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   23h   v1.32.10
node1    Ready    <none>          68m   v1.32.10
node2    Ready    <none>          62m   v1.32.10

    
   https://kubernetes.io/docs/reference/kubectl/docker-cli-to-kubectl/ 
   
   
    155  kubectl get nodes
  156  kubectl run pod1 --image nginx
  157  kubectl get pods
  158  kubectl run pod2 --image nginx
  159  kubectl get pods
  160  kubectl get pods -o wide
  161  kubectl describe pod pod1
  162  kubectl get pods -o wide
    
LAB: 

  452  kubectl get pods
  453  kubectl run webappt --image=nginx --port=80 --labels="app=ticketing,env=testing"
  454  kubectl run webappqa --image=nginx --port=80 --labels="app=ticketing,env=qa"
  455  kubectl run webappprod --image=nginx --port=80 --labels="app=ticketing,env=prod"
  456  kubectl get pods
  457  kubectl get pods -l env=prod
  458  kubectl get pods -l env=ticketing
  459  kubectl get pods -l app=ticketing
  460  kubectl get pods webappqa

LAB: create pod using yml file with the following 
image: nginx   -   nginxdemos/hello:0.4
container port: 80
labels: env: qa, project: delivery
use kubelctl command to filter the pod based on label
Done: Vinoth Baskaran Mohit Madaan,Karthik Krishnan, Mahesh Meesa,Eswari,Balaji
    
      456  kubectl apply -f ./2.demopod.yml
  457  kubectl get pods
  458  kubectl describe pod demopod
  459  kubectl get pods
  460  kubectl describe pod nginx-pod
  461  kubectl port-forward nginx-pod 9999:80
  462  kubectl delete pod demopod
  463  kubectl get pods
  464  kubectl apply -f ./2.demopod.yml
  465  kubectl get pods
    
cat 1.pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app.kubernetes.io/name: myapp
    env: production
spec:
  containers:
  - name: nginx-container
    image: nginx:1-alpine #your app image name
    #image: docker.io/uraccountname/nginx:1-alpine #your app image name
    ports:
      - containerPort: 80
# kubectl apply -f 1.pod.yml
# kubectl get pod nginx-pod
# kubectl describe pod nginx-pod
# kubectl exec nginx-pod -- cat /etc/os-release
# kubectl exec -it nginx-pod -- /bin/sh
# kubectl delete pod nginx-pod

https://www.sysdig.com/blog/kubernetes-limits-requests
    
    

1. Node Monitoring

    Kube-controller-manager monitors node status via node-monitor-period (default: 5s)

    Node Lifecycle Controller detects unhealthy nodes

    A node is marked NotReady after node-monitor-grace-period (default: 40s)


2. Automatic Pod Eviction Timeline
Node crashes → 40s (NotReady) → 5m (Taint added) → Pods rescheduled

    After pod-eviction-timeout (default: 5 minutes), pods are evicted and rescheduled


3. Pod Disruption Behavior
# Pods will be automatically recreated on other nodes IF:
    # 1. They are managed by a controller:
        #    - Deployment
        #    - StatefulSet
        #    - DaemonSet (will recreate on all nodes)
        #    - ReplicaSet#    - Job/CronJob
        
        
   Pods WITHOUT Controllers     
# Pods created directly with `kubectl run` or `kubectl apply` without a controller
# WILL NOT be rescheduled 
kubectl run nginx --image=nginx  # No controller = won't restart 

Day 16 - 8Dec25
kubectl run app1 --image nginx:1.29   or httpd:2.4
kubectl run app1 --image nginx:1.28  or httpd:alpine

cat 5.deployment.yml  #default updatestrategy:RollingUpdate
apiVersion: apps/v1 #version of the API to use 
kind: Deployment #What kind of object we're deploying 
metadata: #information about our object we're deploying 
  name: nginx-deployment #Name of the deployment 
  labels: #A tag on the deployments created 
    app: nginx 
spec: #specifications for our object 
  replicas: 2 #The number of pods that should always be running 
  selector: #which pods the replica set should be responsible for 
    matchLabels: 
      app: nginx #any pods with labels matching this I'm responsible for. 
  template: #The pod template that gets deployed 
    metadata: 
      labels: #A tag on the replica sets created 
        app: nginx 
    spec: 
      containers: 
      - name: nginx-container #the name of the container within the pod 
        image: nginx:1.14.2 #which container image should be pulled 
      # image: nginxdemos/hello:plain-text 
        ports: 
        - containerPort: 80 #the port of the container within the pod 
 
# kubectl apply -f 5.deployment.yml 
# kubectl get pods --all 

kubectl set image deployment/nginx-deployment nginx-container=nginx:1.16.1

 Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  9m     deployment-controller  Scaled up replica set nginx-deployment-7c7cbbddd4 from 0 to 2
  Normal  ScalingReplicaSet  6m26s  deployment-controller  Scaled up replica set nginx-deployment-7c7cbbddd4 from 2 to 4
  Normal  ScalingReplicaSet  11s    deployment-controller  Scaled up replica set nginx-deployment-85c647ffbf from 0 to 1
  Normal  ScalingReplicaSet  11s    deployment-controller  Scaled down replica set nginx-deployment-7c7cbbddd4 from 4 to 3
  Normal  ScalingReplicaSet  11s    deployment-controller  Scaled up replica set nginx-deployment-85c647ffbf from 1 to 2
  Normal  ScalingReplicaSet  2s     deployment-controller  Scaled down replica set nginx-deployment-7c7cbbddd4 from 3 to 2
  Normal  ScalingReplicaSet  2s     deployment-controller  Scaled up replica set nginx-deployment-85c647ffbf from 2 to 3
  Normal  ScalingReplicaSet  1s     deployment-controller  Scaled down replica set nginx-deployment-7c7cbbddd4 from 2 to 1
  Normal  ScalingReplicaSet  1s     deployment-controller  Scaled up replica set nginx-deployment-85c647ffbf from 3 to 4
  Normal  ScalingReplicaSet  0s     deployment-controller  (combined from similar events): Scaled down replica set nginx-deployment-7c7cbbddd4 from 1 to 0

cat 5.1deployRecreate.yml
apiVersion: apps/v1 #version of the API to use
kind: Deployment #What kind of object we're deploying
metadata: #information about our object we're deploying
  name: nginx-deployment #Name of the deployment
  labels: #A tag on the deployments created
    app: nginxd
spec: #specifications for our object
  strategy:
    type: Recreate
  replicas: 4 #The number of pods that should always be running
  selector: #which pods the replica set should be responsible for
    matchLabels:
      app: nginx #any pods with labels matching this I'm responsible for.
  template: #The pod template that gets deployed
    metadata:
      labels: #A tag on the replica sets created
        app: nginx
    spec:
      containers:
      - name: nginx-container #the name of the container within the pod
        image: nginx:1.14.2 #which container image should be pulled
      # image: nginxdemos/hello:plain-text
        ports:
        - containerPort: 80 #the port of the container within the pod

# kubectl apply -f 5.1deployment.yml
# kubectl get pods --all

FullUpdate-strategy -with rollback
apiVersion: apps/v1 #version of the API to use
kind: Deployment #What kind of object we're deploying
metadata: #information about our object we're deploying
  name: nginx-deployment #Name of the deployment
  labels: #A tag on the deployments created
    app: nginxd
spec: #specifications for our object
  strategy:
    type: RollingUpdate
    rollingUpdate: #Update Pods a certain number at a time
      maxUnavailable: 1 #Total number of pods that can be unavailable at once
      maxSurge: 1 #Maximum number of pods that can be deployed above desired state
  replicas: 6 #The number of pods that should always be running
  selector: #which pods the replica set should be responsible for
    matchLabels:
      app: nginx2 #any pods with labels matching this I'm responsible for.
  template: #The pod template that gets deployed
    metadata:
      labels: #A tag on the replica sets created
        app: nginx2
    spec:
      containers:
        - name: nginx-container #the name of the container within the pod
          #image: nginx:1.16.1 #which container image should be pulled
          image: nginxdemos/hello:0.4-plain-text #which container image should be pulled
          ports:
            - containerPort: 80 #the port of the container within the pod


#kubectl apply -f deploy.yaml --record=true
  #       observe strategy type as rollingUpdate:
  # update image to new version
  #        kubectl set image deploy nginx-deployment nginx-container=nginx:latest1 --record
  #         kubectl rollout status deployment/nginx-deployment
  #        kubectl get deploy

  # rollback
  #       kubectl set image deploy nginx-deploy nginx-container=nginx:latest --record
  #       kubectl rollout status deployment/nginx-deployment
  #       kubectl rollout history deployment/nginx-deployment
  #       kubectl rollout undo deployment/nginx-deployment
  #       kubectl rollout undo deployment/nginx-deployment --to-revision=1
  #      kubectl rollout status deployment/nginx-deployment
  #     kubectl scale deploy nginx-deploy --replicas=5
  #     kubectl get deploy
  #     kubectl get po

  #      kubectl scale deploy nginx-deploy --replicas=1
  #     kubectl get deploy
  #      kubectl get po

  #     kubectl delete -f deploy.yml
  #     https://kubernetes.io/docs/concepts/workloads/controllers/deployment/


Service - clusterIP
apiVersion: apps/v1 #version of the API to use
kind: Deployment #What kind of object we're deploying
metadata: #information about our object we're deploying
  name: nginx-deployment #Name of the deployment
  labels: #A tag on the deployments created
    app: nginx
spec: #specifications for our object
  replicas: 2 #The number of pods that should always be running
  selector: #which pods the replica set should be responsible for
    matchLabels:
      app: nginxapp #any pods with ALL labels matching this I'm responsible for.
  template: #The pod template that gets deployed
    metadata:
      labels: #A tag on the replica sets created
        app: nginxapp
    spec:
      containers:
      - name: nginx-container #the name of the container within the pod
        image: nginxdemos/hello:plain-text #which container image should be pulled
        #image: nginxdemos/hello:plain-text #which container image should be pulled
        ports:
        - containerPort: 80 #the port of the container within the pod

---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  selector:
    app: nginxapp
  ports:
  - port: 8090 # reach the pod service
    targetPort: 80 #container port app service provided


    #kubectl.exe apply -f .\6.1clusteripservice.yml
    # kubectl.exe get deploy
    # kubectl.exe get svc
    # curl http://localhost:8090
    # kubectl exec -it nginx-deployment-56f9df4dd9-6x4hs -- sh
      #curl http://10.96.31.188:8090
      #curl http://myservice:8090
      #exit
    #kubectl port-forward svc/myservice 8090:8090 # will always forward to only one container pod
    #curl http://localhost:8090 
    
       #kubectl get serviceendpoints myservice
    #kubectl describe svc myservice
    #kubectl.exe delete -f .\clusteripservice.yml 
    
#without service we can use expose command
# kubectl expose deploy nginx-deployment --type=ClusterIP --name=ingress-nginx --port=8010 --target-port=80 --protocol=TCP

NodePort.yml

apiVersion: apps/v1 #version of the API to use
kind: Deployment #What kind of object we're deploying
metadata: #information about our object we're deploying
  name: nginx-deployment #Name of the deployment
  labels: #A tag on the deployments created
    app: nginx
spec: #specifications for our object
  replicas: 2 #The number of pods that should always be running
  selector: #which pods the replica set should be responsible for
    matchLabels:
      app: nginx #any pods with labels matching this I'm responsible for.
  template: #The pod template that gets deployed
    metadata:
      labels: #A tag on the replica sets created
        app: nginx
    spec:
      containers:
        - name: nginx-container #the name of the container within the pod
          image: nginxdemos/hello:plain-text #which container image should be pulled
          ports:
            - containerPort: 80 #the port of the container within the pod
---
apiVersion: v1 #version of the API to use
kind: Service #What kind of object we're deploying
metadata: #information about our object we're deploying
  name: nodeport-nginx #Name of the service
spec: #specifications for our object
  type: NodePort #exposes a service on each node’s IP address on a specific port
  ports: #
    - name: http
      port: 80 # service port
      targetPort: 80 #container port
      nodePort: 30001 #port range 30000-32767
      protocol: TCP
  selector: #Label selector used to identify pods
    app: nginx
# kubectl apply -f 6.2nodeport.yaml
# kubectl get nodes -o wide
# curl http://node1-ip-address:30001 execute multiple times observer diffrent pods responding



AKS.yml >>>k8s app from AKS deploy guide

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rabbitmq
spec:
  serviceName: rabbitmq
  replicas: 1
  selector:
    matchLabels:
      app: rabbitmq
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      nodeSelector:
        "kubernetes.io/os": linux
      containers:
      - name: rabbitmq
        image: mcr.microsoft.com/mirror/docker/library/rabbitmq:3.10-management-alpine
        ports:
        - containerPort: 5672
          name: rabbitmq-amqp
        - containerPort: 15672
          name: rabbitmq-http
        env:
        - name: RABBITMQ_DEFAULT_USER
          value: "username"
        - name: RABBITMQ_DEFAULT_PASS
          value: "password"
        resources:
          requests:
            cpu: 10m
            memory: 128Mi
          limits:
            cpu: 250m
            memory: 256Mi
        volumeMounts:
        - name: rabbitmq-enabled-plugins
          mountPath: /etc/rabbitmq/enabled_plugins
          subPath: enabled_plugins
      volumes:
      - name: rabbitmq-enabled-plugins
        configMap:
          name: rabbitmq-enabled-plugins
          items:
          - key: rabbitmq_enabled_plugins
            path: enabled_plugins
---
apiVersion: v1
data:
  rabbitmq_enabled_plugins: |
    [rabbitmq_management,rabbitmq_prometheus,rabbitmq_amqp1_0].
kind: ConfigMap
metadata:
  name: rabbitmq-enabled-plugins
---
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq
spec:
  selector:
    app: rabbitmq
  ports:
    - name: rabbitmq-amqp
      port: 5672
      targetPort: 5672
    - name: rabbitmq-http
      port: 15672
      targetPort: 15672
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: order-service
  template:
    metadata:
      labels:
        app: order-service
    spec:
      nodeSelector:
        "kubernetes.io/os": linux
      containers:
      - name: order-service
        image: ghcr.io/azure-samples/aks-store-demo/order-service:latest
        ports:
        - containerPort: 3000
        env:
        - name: ORDER_QUEUE_HOSTNAME
          value: "rabbitmq"
        - name: ORDER_QUEUE_PORT
          value: "5672"
        - name: ORDER_QUEUE_USERNAME
          value: "username"
        - name: ORDER_QUEUE_PASSWORD
          value: "password"
        - name: ORDER_QUEUE_NAME
          value: "orders"
        - name: FASTIFY_ADDRESS
          value: "0.0.0.0"
        resources:
          requests:
            cpu: 1m
            memory: 50Mi
          limits:
            cpu: 75m
            memory: 128Mi
        startupProbe:
          httpGet:
            path: /health
            port: 3000
          failureThreshold: 5
          initialDelaySeconds: 20
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 3000
          failureThreshold: 3
          initialDelaySeconds: 3
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          failureThreshold: 5
          initialDelaySeconds: 3
          periodSeconds: 3
      initContainers:
      - name: wait-for-rabbitmq
        image: busybox
        command: ['sh', '-c', 'until nc -zv rabbitmq 5672; do echo waiting for rabbitmq; sleep 2; done;']
        resources:
          requests:
            cpu: 1m
            memory: 50Mi
          limits:
            cpu: 75m
            memory: 128Mi
---
apiVersion: v1
kind: Service
metadata:
  name: order-service
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 3000
    targetPort: 3000
  selector:
    app: order-service
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: product-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: product-service
  template:
    metadata:
      labels:
        app: product-service
    spec:
      nodeSelector:
        "kubernetes.io/os": linux
      containers:
      - name: product-service
        image: ghcr.io/azure-samples/aks-store-demo/product-service:latest
        ports:
        - containerPort: 3002
        env:
        - name: AI_SERVICE_URL
          value: "http://ai-service:5001/"
        resources:
          requests:
            cpu: 1m
            memory: 1Mi
          limits:
            cpu: 2m
            memory: 20Mi
        readinessProbe:
          httpGet:
            path: /health
            port: 3002
          failureThreshold: 3
          initialDelaySeconds: 3
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 3002
          failureThreshold: 5
          initialDelaySeconds: 3
          periodSeconds: 3
---
apiVersion: v1
kind: Service
metadata:
  name: product-service
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 3002
    targetPort: 3002
  selector:
    app: product-service
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: store-front
spec:
  replicas: 1
  selector:
    matchLabels:
      app: store-front
  template:
    metadata:
      labels:
        app: store-front
    spec:
      nodeSelector:
        "kubernetes.io/os": linux
      containers:
      - name: store-front
        image: ghcr.io/azure-samples/aks-store-demo/store-front:latest
        ports:
        - containerPort: 8080
          name: store-front
        env:
        - name: VUE_APP_ORDER_SERVICE_URL
          value: "http://order-service:3000/"
        - name: VUE_APP_PRODUCT_SERVICE_URL
          value: "http://product-service:3002/"
        resources:
          requests:
            cpu: 1m
            memory: 200Mi
          limits:
            cpu: 1000m
            memory: 512Mi
        startupProbe:
          httpGet:
            path: /health
            port: 8080
          failureThreshold: 3
          initialDelaySeconds: 5
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          failureThreshold: 3
          initialDelaySeconds: 3
          periodSeconds: 3
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          failureThreshold: 5
          initialDelaySeconds: 3
          periodSeconds: 3
---
apiVersion: v1
kind: Service
metadata:
  name: store-front
spec:
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30002
  selector:
    app: store-front
  type: NodePort



Day 17 - 9Dec25
singlenode k8s cluster
https://kind.sigs.k8s.io/docs/user/quick-start/
https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download

7.1emptydir.yml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: nginxdemos/hello:plain-text
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir:
      sizeLimit: 500Mi


# kubectl apply -f 7.1.emptyDir.yml
# kubectl exec test-pd -it -- /bin/sh 
# pwd
# ls
# ls cache/
# echo test > /cache/textfile 
# ls /cache
# cat /cache/textfile 
# exit

On the node pod running find uid of the pod - kubectl get pod test-pd -o json|grep uid
/var/lib/kubelet/pods/UID-Of-Pod/volumes/kubernetes.io~empty-dir/cache-volume

cat ./7.2hostpath.yml
apiVersion: v1
kind: Pod
metadata:
  name: hostpathpod
spec:
  containers:
  - image: nginxdemos/hello:plain-text
    name: test-container
    volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: test-hostpath
  volumes:
  - name: test-hostpath
    hostPath:
      path: /testdata
      type: DirectoryOrCreate #if dir not available create new
    
    check in pod hosted node
      
cat 7.11bb-sv.yml
apiVersion: v1
kind: Pod
metadata:
  name: bbsvpod
spec:
  containers:
  - image: busybox
    name: busy
    args: [/bin/sh, -c, 'while true; do echo $(date +"%Y-%m-%d %H:%M:%S") >> /busy/index.html; sleep 2; done']
    volumeMounts:
    - mountPath: /busy
      name: test
  - image: nginx
    name: box
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /usr/share/nginx/html
      name: test
  volumes:
  - name: test
    emptyDir: {}

#multicontainer shared volume mapping
# kubectl apply -f 12.1bb-sv.yml
# kubectl get pods
# kubectl exec -it bbsvpod -c busy -- cat /busy/index.html
# kubectl get pods all -o wide
# curl http://podip
# kubectl delete -f 7.11bb-sv.yml

